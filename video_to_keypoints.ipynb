{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIDEO_ID</th>\n",
       "      <th>SIGNER_ID</th>\n",
       "      <th>IS_SENTENCE</th>\n",
       "      <th>SIGN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MVI_3949</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MVI_3950</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MVI_3951</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MVI_3952</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MVI_3953</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>MVI_5264</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes, make sure you're getting plenty of rest a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>MVI_5266</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Try to eat a balanced diet and get some light ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>MVI_5267</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Try to eat a balanced diet and get some light ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>MVI_5268</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Okay, I'll do my best. Thanks, doctor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>MVI_5270</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Okay, I'll do my best. Thanks, doctor.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1071 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      VIDEO_ID SIGNER_ID  IS_SENTENCE   \n",
       "0     MVI_3949         1            0  \\\n",
       "1     MVI_3950         1            0   \n",
       "2     MVI_3951         1            0   \n",
       "3     MVI_3952         1            0   \n",
       "4     MVI_3953         1            0   \n",
       "...        ...       ...          ...   \n",
       "1066  MVI_5264         1            1   \n",
       "1067  MVI_5266         1            1   \n",
       "1068  MVI_5267         1            1   \n",
       "1069  MVI_5268         1            1   \n",
       "1070  MVI_5270         1            1   \n",
       "\n",
       "                                                   SIGN  \n",
       "0                                                Health  \n",
       "1                                                 Human  \n",
       "2                                                  Head  \n",
       "3                                                  Mind  \n",
       "4                                                 Brain  \n",
       "...                                                 ...  \n",
       "1066  Yes, make sure you're getting plenty of rest a...  \n",
       "1067  Try to eat a balanced diet and get some light ...  \n",
       "1068  Try to eat a balanced diet and get some light ...  \n",
       "1069             Okay, I'll do my best. Thanks, doctor.  \n",
       "1070             Okay, I'll do my best. Thanks, doctor.  \n",
       "\n",
       "[1071 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"SignTlak Dataset - Copy.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "\tmin_detection_confidence=0.5,\n",
    "\tmin_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Initializing the drawing utils for drawing the facial landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"G:/DCIM/100CANON/\"\n",
    "video_folder = os.listdir(video_path)\n",
    "for index, row in df.iterrows():\n",
    "    video_id, sign = row['VIDEO_ID'], row['SIGN']\n",
    "    video = video_id + \".MOV\"\n",
    "\n",
    "    if video in video_folder:\n",
    "\n",
    "        curr_landmarks = []       \n",
    "\n",
    "        video_dir = video_path + video\n",
    "        capture = cv2.VideoCapture(video_dir)\n",
    "\n",
    "        # Initializing current time and precious time for calculating the FPS\n",
    "        previousTime = 0\n",
    "        currentTime = 0\n",
    "\n",
    "        while capture.isOpened():\n",
    "\t        # capture frame by frame\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "\t        # resizing the frame for better view\n",
    "            frame = cv2.resize(frame, (800, 600))\n",
    "            \n",
    "            # Converting the from BGR to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t        # Making predictions using holistic model\n",
    "\t        # To improve performance, optionally mark the image as not writeable to\n",
    "\t        # pass by reference.\n",
    "            \n",
    "            image.flags.writeable = False\n",
    "            results = holistic_model.process(image)\n",
    "            image.flags.writeable = True\n",
    "\n",
    "\t        # Converting back the RGB image to BGR\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "\t\t\t# Process pose\n",
    "            pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "            face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "            frame_pose = np.concatenate([pose, face, lh, rh])\n",
    "            curr_landmarks.append(frame_pose)\n",
    "            \n",
    "\t\t\t\n",
    "\n",
    "\t        # Drawing the Facial Landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "\t            image,\n",
    "\t            results.face_landmarks,\n",
    "\t            mp_holistic.FACEMESH_CONTOURS,\n",
    "\t            mp_drawing.DrawingSpec(\n",
    "\t\t        color=(255,0,255),\n",
    "\t\t        thickness=1,\n",
    "\t\t        circle_radius=1\n",
    "\t        ),\n",
    "\t\n",
    "            mp_drawing.DrawingSpec(\n",
    "\t\t        color=(0,255,255),\n",
    "\t\t        thickness=1,\n",
    "\t\t        circle_radius=1\n",
    "\t        )\n",
    "\t        )\n",
    "\n",
    "\t        # Drawing Right hand Land Marks\n",
    "            mp_drawing.draw_landmarks(\n",
    "\t            image,\n",
    "\t            results.right_hand_landmarks,\n",
    "\t            mp_holistic.HAND_CONNECTIONS\n",
    "\t        )\n",
    "\n",
    "\t        # Drawing Left hand Land Marks\n",
    "            mp_drawing.draw_landmarks(\n",
    "\t            image,\n",
    "\t            results.left_hand_landmarks,\n",
    "\t            mp_holistic.HAND_CONNECTIONS\n",
    "\t        )\n",
    "\n",
    "\t        # Drawing pose Land Marks\n",
    "            mp_drawing.draw_landmarks(\n",
    "\t            image,\n",
    "\t            results.pose_landmarks,\n",
    "\t            mp_holistic.POSE_CONNECTIONS\n",
    "\t        )\n",
    "\t\n",
    "\t        # Calculating the FPS\n",
    "            currentTime = time.time()\n",
    "            fps = 1 / (currentTime-previousTime)\n",
    "            previousTime = currentTime\n",
    "\t\n",
    "\t        # Displaying FPS on the image\n",
    "            cv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "\t        # Display the resulting image\n",
    "            cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "\t        # Enter key 'q' to break the loop\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            # When all the process is done\n",
    "            # Release the capture and destroy all windows\n",
    "        \n",
    "        y.append(sign)\n",
    "        X.append(curr_landmarks)\n",
    "\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # print(X)\n",
    "        # print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059\n",
      "1059\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_9984\\55319187.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array(X)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"wed_X.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"wed_y.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(subarray) for subarray in X)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wed_X = np.load(\"wed_X.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(subarray) for subarray in wed_X)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1662"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wed_X[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= wed_X[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(x)\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "x = np.asarray(x).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 and 3: Pad sublists with arrays of zeros to match the maximum length\n",
    "for sublist in wed_X:\n",
    "    \n",
    "    while len(sublist) < max_length:\n",
    "        sublist.append(np.array([0] * 1662))\n",
    "    sublist = np.array(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X  = wed_X[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wed_X[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n"
     ]
    }
   ],
   "source": [
    "print(len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1059, 372)]       0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1059, 512)         190976    \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 1059, 512)        1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1059, 512)         0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1059, 512)         0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1059, 256)         131328    \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 1059, 256)        512       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1059, 256)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1059, 256)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 250)               507000    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 870)               218370    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,049,210\n",
      "Trainable params: 1,049,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# include early stopping and reducelr\n",
    "def get_callbacks():\n",
    "    return [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience = 10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = \"val_accuracy\",\n",
    "            factor = 0.5,\n",
    "            patience = 3\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "# a single dense block followed by a normalization block and relu activation\n",
    "def dense_block(units, name):\n",
    "    fc = layers.Dense(units)\n",
    "    norm = layers.LayerNormalization()\n",
    "    act = layers.Activation(\"relu\")\n",
    "    drop = layers.Dropout(0.1)\n",
    "    return lambda x: drop(act(norm(fc(x))))\n",
    "\n",
    "# the lstm block with the final dense block for the classification\n",
    "def classifier(lstm_units):\n",
    "    lstm = layers.LSTM(lstm_units)\n",
    "    out = layers.Dense(870, activation=\"softmax\")\n",
    "    return lambda x: out(lstm(x))\n",
    "# choose the number of nodes per layer\n",
    "encoder_units = [512, 256] # tune this\n",
    "lstm_units = 250 # tune this\n",
    "\n",
    "#define the inputs (ragged batches of time series of landmark coordinates)\n",
    "inputs = tf.keras.Input(shape=((1059, 372)), ragged=True)\n",
    "\n",
    "# dense encoder model\n",
    "x = inputs\n",
    "for i, n in enumerate(encoder_units):\n",
    "    x = dense_block(n, f\"encoder_{i}\")(x)\n",
    "\n",
    "# classifier model\n",
    "out = classifier(lstm_units)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m OneHotEncoder\n\u001b[0;32m      2\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m y_encoded \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mfit_transform(np\u001b[39m.\u001b[39marray(y)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_encoded = encoder.fit_transform(np.array(y).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_padded = tf.keras.preprocessing.sequence.pad_sequences(wed_X, padding='post')\n",
    "\n",
    "# Convert X and y datasets to TensorFlow tensors\n",
    "# X_dataset = tf.convert_to_tensor(X_padded, dtype=tf.float32)\n",
    "# y_encoded = tf.convert_to_tensor(y_encoded, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train \u001b[39m=\u001b[39m wed_X\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49muint8)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "X_train = wed_X.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.88 GiB for an array with shape (1059, 372, 1662) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray([np\u001b[39m.\u001b[39;49marray(val) \u001b[39mfor\u001b[39;49;00m val \u001b[39min\u001b[39;49;00m wed_X])\n\u001b[0;32m      2\u001b[0m \u001b[39m# y_train = np.array([np.array(val) for val in y_train])\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.88 GiB for an array with shape (1059, 372, 1662) and data type float64"
     ]
    }
   ],
   "source": [
    "x_train = np.array([np.array(val) for val in wed_X])\n",
    "# y_train = np.array([np.array(val) for val in y_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcast(wed_X , dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "x_train = tf.cast(wed_X , dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m      4\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msparse_top_k_categorical_accuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[39m# fit the model with 100 epochs iteration\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mfit(wed_X,\n\u001b[0;32m      8\u001b[0m           y_encoded,\n\u001b[0;32m      9\u001b[0m           callbacks \u001b[39m=\u001b[39;49m get_callbacks(),\n\u001b[0;32m     10\u001b[0m           epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\",\"sparse_top_k_categorical_accuracy\"])\n",
    "# fit the model with 100 epochs iteration\n",
    "model.fit(wed_X,\n",
    "          y_encoded,\n",
    "          callbacks = get_callbacks(),\n",
    "          epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(1059, 372)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(870, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Health': 0, 'Human': 1, 'Head': 639, 'Mind': 3, 'Brain': 4, 'Live': 5, 'Face': 640, 'Eye': 643, 'Ear': 8, 'Nose': 9, 'Mouth': 10, 'Teeth': 11, 'Beard': 12, 'Hand': 13, 'Feet': 14, 'Blood': 758, 'Breast': 726, 'Penis': 714, 'Anus': 711, 'Flatulence': 19, 'Spit': 20, 'Sweat': 21, 'Sleep': 22, 'Lie down': 790, 'Bath': 952, 'Smell': 25, 'Hear': 26, 'Blind': 27, 'Sick': 798, 'Infection': 664, 'Deaf': 661, 'Cut': 716, 'Sore': 32, 'Diarrhea': 723, 'Cold': 689, 'Convulsion': 36, 'Headache': 37, 'Chicken pox': 38, 'Malaria': 39, 'Tuberclosis': 40, 'Ebola': 41, 'Cancer': 42, 'HIV': 702, 'AIDS': 663, 'Transmit': 45, 'STI': 701, 'Mental': 47, 'Disabled': 48, 'Crippled': 49, 'Hospital': 803, 'Surgery ': 51, 'Sex': 667, 'gender': 53, 'Lesbian': 54, 'Semen': 56, 'Erection': 57, 'Condom': 669, 'Vaginal fluid': 59, 'Menstruation': 60, 'Pregnant': 717, 'Birth': 715, 'Breastfeed': 63, 'Female circumcision': 64, 'Female masturbation ': 65, 'Female masturbation': 66, 'Abortion': 67, 'Smoking': 68, 'Wee': 69, 'Medicine': 731, 'Danger': 72, 'Emergency': 73, 'Challenge Authority ': 74, 'Curious': 75, 'Being idle': 76, \"You're funny\": 79, 'Let down': 78, 'Low intelligence': 80, 'Drunk': 81, 'Difficult to change': 82, 'Heart beating fast': 83, 'Hearing is dead': 84, 'Toothless': 85, 'Shaved head': 86, 'Big butt': 87, 'Locality': 88, 'Face to face': 89, 'Flirting': 90, 'I love you': 91, 'Easy talk': 92, 'Relieved': 93, 'Smelling something good': 94, 'Variety of things': 95, 'Pick somebody': 96, 'force to join': 97, 'Spread information': 98, 'Thinking hard': 99, 'keep in mind': 100, 'evict': 101, 'Suspend': 102, 'Backbiting': 103, \"Don't bother me\": 104, 'Refusing': 105, 'Out of here': 106, 'Getting away': 107, 'Peeping': 108, 'Copying someone': 109, 'Not interested': 110, 'you are Interfering': 111, 'Not my problem': 112, \"Don't like you\": 113, 'Sick of you': 114, 'Hard stare': 115, 'Gazing': 116, 'Fed up': 117, 'I told you so': 118, 'What did I tell you': 119, 'Keep for revenge': 120, 'Fifty fifty': 121, 'Idea': 122, 'Experience ': 123, 'Like': 124, 'Favourite ': 125, 'Know': 671, 'Misunderstand': 127, 'Obey': 128, 'Before': 598, 'Forget': 130, 'Doubt': 131, 'Lie': 132, 'Confident': 133, 'Dream': 134, 'Wish': 135, 'Focus': 136, 'Consider': 137, 'Believe': 138, 'Agree': 139, 'Inform': 140, 'Want': 777, 'Reason': 142, 'Sure': 143, 'So': 144, 'Very': 145, 'Pressure': 146, 'Cause': 147, 'Happen': 788, 'Memory': 149, 'Remind': 150, 'Meaning': 151, 'Scream': 152, 'Emotion': 153, 'Sad': 154, 'Angry': 155, 'Fear': 156, 'Disappointed ': 157, 'Frustrate ': 158, 'Jealous': 159, 'Lonely': 161, 'Nervous': 749, 'Worry': 163, 'Hate': 164, 'Desire': 166, 'Hurt': 646, 'Character': 168, 'Best': 169, 'Important': 170, 'Boast': 171, 'Fair': 172, 'Funny': 174, 'Humble': 175, 'Kind': 176, 'Lucky': 177, 'Lazy': 178, 'Patient': 179, 'Proud': 180, 'Respect': 181, 'Selfish': 182, 'Stubborn': 183, 'Stupid': 184, 'Wicked': 185, 'Vain': 186, 'Wise': 187, 'Strict': 188, 'Embarrased': 189, 'Creative': 190, 'Apologize': 191, 'Why are you here': 193, 'Show me where the pain begins and where it ends': 195, 'How long have you had the pain': 197, 'Why do you think you have the pain': 199, 'I have a terrible pain right here': 201, 'I have had the pain for about fifteen minutes': 203, 'About three days': 205, 'It has been coming and going for five weeks': 207, 'I think it is an old injury coming back': 209, 'I bumped myself but not very hard': 210, 'You are the doctor you tell me': 212, 'What were you doing when the pain began': 214, 'Is the pain dull or sharp': 216, 'How long does your pain last': 218, 'Do you have cramping to abdominal pain': 220, 'I was bending over to pick up something': 222, 'I woke up and the pain was there': 224, 'I was standing when the pain came': 226, 'I was sitting down at work and the pain came': 228, 'I was walking and the pain began when I started running ': 230, 'No this is the first time': 232, 'Yes but it was different. It hurts more this time': 234, 'The pain is very sharp when I move my arm or breathe deeply': 236, 'The pain is throbbing, heavy and constant': 238, 'Several seconds': 240, 'Thirty minutes': 242, 'What relieves your pain': 244, 'Does bowel movement relieve your pain': 246, 'Do specific foods cause your pain': 248, 'Do acidic foods cause discomfort': 250, 'Standing is better': 252, 'No matter what position I am in it still hurts': 254, 'It is much worse': 256, 'If I donâ€™t eat anything I feel better': 258, 'Yes orange juice causes me pain': 260, 'After I eat a meal it hurts': 262, 'The pain is much worse in the morning': 264, 'If I stand it hurts': 266, 'Yes my father and mother': 268, 'Yes both my brother and sister': 270, 'Yes my aunt and uncle': 272, 'I hurt my neck in a car accident last month': 274, 'I fell about two weeks ago': 276, 'How did your car accident happen': 278, 'What is the last thing you remember': 280, 'Where were you sitting in the car': 282, 'Did you strike your head': 284, 'I cannot remember': 286, 'I lost control of the car and hit an animal': 288, 'I fell asleep at the wheel': 290, 'Yes I was wearing a seatbelt': 292, 'I was sitting infront beside the driver': 294, 'Do you have a headache now': 296, 'Does it hurt when I touch you here': 298, 'Tell me if it hurts': 300, 'If I take a deep breath I feel a sudden pain': 302, 'Yes it hurts': 304, 'When did you fall': 306, 'Did you land on a soft or hard surface': 308, 'Which part of your body hit first': 310, 'Can you walk around': 312, 'Can you move your fingers and toes': 314, 'Hard concrete': 316, 'I landed on my elbow first': 318, 'Yes I can walk around but it hurts': 320, 'What caused your pain': 322, 'Since when': 324, 'Yes I have been coughing every morning': 326, 'Yes hay fever': 328, 'Are there any medcines you cannot take': 330, 'I get stomach cramps and vomit': 332, 'What is your name': 334, 'Can you call a doctor': 336, 'Can you call my family': 338, 'Can I have more pain medications': 340, 'When can I go home': 342, 'Can you tell me where it hurts': 344, 'How long have you been experiencing these symptoms': 346, 'The results of your tests are in. I need to discuss them with you': 348, 'We recommend you stay overnight for observation': 350, 'You need to take this medication in the morning afternoon and evening': 352, 'I am a professional doctor': 354, 'Please take a deep breath': 356, 'I have a change in the size of my breast ': 357, 'Fluids discharge from my nipples': 358, 'I have a rash on my skin': 359, 'I have a runny nose': 360, 'I cough a lot': 361, 'My head aches': 362, 'I feel pain when I pee': 363, 'There is unusual discharge from my private parts': 364, 'My eye itches': 365, 'My body itches': 366, 'I have been sneezing persistently ': 367, 'I feel very thirsty ': 368, 'I urinate frequently ': 369, 'I feel tired': 370, 'My wounds heal slowly': 371, 'I have blurred vision': 372, 'I feel nausea': 373, 'I experience lack of appetite': 374, 'I cough out blood': 375, 'I feel very hot': 376, 'I sweat profusely ': 377, 'I feel very cold': 378, 'I have pains in my muscle': 379, 'I have been vomitting ': 380, 'I keep forgetting things': 381, 'My heart beats rapidly': 382, 'Feeling of general uneasiness': 383, 'I have chest pain': 384, 'Patchy hair loss': 385, 'Weight loss': 386, 'I have pain in my joints': 387, 'I have a backache': 388, 'I have a stiff neck': 389, 'I have difficulty in breathing': 390, 'I have a sore in my throat': 391, 'I have a burning sensation': 392, 'Can you point to where it hurts': 393, 'I need to take your blood pressure': 394, 'Open your mouth and say ah': 395, 'Can you follow my finger with your eyes': 396, 'Please lie down on the examination table': 397, 'I need to check your pulse': 398, 'Can you show me where the pain is on this board': 399, 'Do you have any medical condition': 400, 'I need to examine your ears nose and throat': 401, 'Please remove your shirt for a chest examination': 402, 'Can you sign your name on this form': 403, 'Please drink this water before te test ': 404, 'I need to measure your temperature': 405, 'Can you show me how you take your medications': 406, 'Please put on this hospital gown': 407, 'I need to listen to your heart rate': 408, 'Can you show me how you move your injured limb': 409, 'Please read the letters on this eye chart': 410, 'I need to perform a physical examination': 411, 'Can you show me your previous medical records': 413, 'Please remove your shoes and socks for a foot examination': 414, 'Can you show me how you use this device': 415, 'Please tilt your head back for a throat examination': 416, 'I need to take a urine sample for testing': 417, 'Can you show mehow you do your breathing exercises': 418, 'Please close your eyes and tell me what you feel': 419, 'I need to perform a skin examination': 420, 'Can you show me your medication schedule': 421, 'Please stand up and walk towards me': 422, 'I need to measure your height and weight': 423, 'Please hold still while I examine your  wound': 424, 'Do you have any questions or concerns': 425, 'Can you write that down please': 426, \"I am allergic to this medication don't give it to me\": 427, 'Can I have an interpreter during my appointment': 428, \"I don't understand can you explain it to me again\": 429, 'How long will it take for me to recover.': 430, 'Can you sign slowly': 431, 'Can you talk slowly': 432, 'Thank you for your help': 433, 'Okay I will try my best to follow your instructions ': 434, 'Sure I can show you where it hurts': 435, 'I have never had any allergiesto medications before': 436, 'I am a bit nervous about getting a shot but I understand it Is necessary ': 437, 'I do not feel comfortable taking off my clothes for an exam': 438, 'I am having trouble seeing your finger, can you please move it slower': 439, 'I am feeling a bit dizzy lying down is that normal': 440, 'I have a fast heart is that something to worry about': 441, 'The pain is around here': 442, 'I am scared of needles can you please be gentle': 443, 'Yes I have diabetes and high blood pressure': 444, 'I am having trouble hearing you can you please speak louder': 445, 'I am feeling cold can you put my shirt back on': 446, 'I am sorry I do not know how to sign my name': 447, 'Why do I need to drink waterbefore the test': 448, 'I usually take my medication after breakfast': 449, 'I do not remember all the details of my medical history can you help me': 450, 'Can I look away when you give me the shot': 451, 'How old are you': 452, 'What symptoms are you experiencing': 453, 'Have you had a fever recently': 454, 'Are you experiencing any pain': 455, 'Have you noticed any lumps or bumps': 456, 'Are you experiencing any difficulty breathing': 457, 'Are you experiencing any chest pain': 458, 'Are you experiencing any changes in your vision': 461, 'Have you noticed any changes in your skin': 460, 'Have you noticed any changes in your hearing': 462, 'Are you experiencing any dizziness or lightheadedness': 463, 'Are you experiencing any headaches': 464, 'Are you experiencing any digestive issues': 465, 'Have you noticed any changes in your appetite': 466, 'Are you experiencing any difficulty swallowing': 467, 'Have you noticed any changes in your urine or bowel movements': 468, 'Are you experiencing any difficulty with your balance or coordination': 469, 'Are you experiencing any numbness or tingling': 470, 'Have you noticed any changes in your menstrual cycle': 471, 'Are you experiencing any problems with your sexual function': 472, 'Are there any vaccines or preventative measures I should consider to avoid future illness': 473, 'What re the possible lon-term effects of my condition': 474, 'Are there any potential interactions with my current medications and other supplements': 475, 'How can I manage my symptoms while I wait for my appointment': 476, 'Are there any self-care techniques that can help me feel better during my recovery process': 477, 'I have a headache': 478, 'How long have you had the headache. Have you taken anything for it': 479, 'I have a sore throat.': 480, 'Do you have any other symptoms, such as a fever or cough': 481, 'How long have you had the cough? Is it productive or dry?': 482, 'I have a fever': 483, 'Have you taken your temperature. What other symptoms do you have': 484, 'I have a rash': 485, 'How long have you had diarrhea? Have you been eating or drinking anything unusual?': 486, 'I have constipation': 487, 'How long have you been constipated? Have you made any changes to your diet or routine?': 488, 'I have a stomachache': 520, 'Where is the pain located? Does it come and go, or is it constant': 490, 'I have a sprained ankle.': 491, 'How did you injure your ankle? Have you tried icing and elevating it?': 492, 'I have high blood pressure': 493, 'How long have you known about your high blood pressure? Are you taking any medication for it?': 494, 'I have diabetes': 495, 'Are you experiencing any symptoms related to your diabetes, such as increased thirst or frequent urination': 496, 'I have asthma': 497, 'How often do you experience asthma symptoms? Are you using an inhaler regularly': 498, 'What are you allergic to? Have you tried any over-the-counter allergy medication': 499, 'How long have you had the back pain? Is it a sharp or dull pain?': 502, 'I have back pain': 501, 'How long have you had the cold? Are you experiencing any other symptoms, such as a sore throat or fever': 504, 'Have you been experiencing symptoms such as pain or burning when you urinate, or an urgent need to urinate?': 505, 'Are you experiencing symptoms such as itching, burning, or discharge?': 506, 'I have a sore back': 507, 'Are you experiencing any pain or discomfort from the cold sore? Have you tried any over-the-counter treatments': 508, 'I have an ear infection': 509, 'Have you been experiencing symptoms such as ear pain or discharge': 510, 'I have an upset stomach': 511, 'I have a toothache': 512, 'Are you experiencing any swelling or sensitivity to hot or cold temperatures': 513, 'I have a migraine': 514, 'I have a sprained wrist.': 515, 'I have a cold and a cough.': 516, \"I've been having a headache for a few days now\": 517, 'I think I have a cold.': 518, 'What symptoms are you experiencing? Have you had a fever or cough?': 519, 'I think I have the flu': 521, 'What symptoms are you experiencing? Have you had a fever or body aches?': 522, 'I have a rash on my arm.': 523, 'When did the rash appear? Does it itch or hurt?': 524, 'I have been feeling tired all the time': 525, 'Have you been sleeping well? Have you made any recent changes to your diet or routine?': 526, 'I have a cough and a runny nose': 527, 'I have an earache.': 528, 'Is the pain in one or both ears? ': 530, 'Last night': 531, 'Yesterday': 532, 'Today': 793, 'Last week': 534, 'This week': 535, 'This morning': 536, 'This afternoon': 537, 'This evening': 538, 'Three days ago': 539, 'Six weeks ago': 540, 'A long time ago': 541, 'A while ago': 542, 'Monday': 543, 'Tuesday': 544, 'Wednesday': 545, 'Thursday': 546, 'Saturday ': 547, 'Sunday': 548, 'One': 549, 'Two': 550, 'Three': 551, 'Four': 552, 'Five': 553, 'Six ': 554, 'Seven': 555, 'Eight': 556, 'Nine': 557, 'Ten': 558, 'A': 559, 'B': 560, 'C': 561, 'D': 562, 'E ': 563, 'F': 564, 'G': 565, 'H': 566, 'I': 905, 'J': 568, 'K': 569, 'L': 570, 'M': 571, 'N': 572, 'O': 573, 'P': 574, 'Q': 575, 'R': 576, 'S': 577, 'T': 578, 'U': 579, 'V': 580, 'W': 581, 'X': 582, 'Y': 583, 'Z': 584, 'About': 585, 'Almost': 586, 'Years': 587, 'Months': 588, 'Weeks': 589, 'Days': 590, 'Hours': 591, 'Minutes': 592, 'Injury': 594, 'Doctor': 691, 'Every': 596, 'Do': 597, 'You': 910, 'Your': 911, 'Abdomen': 601, 'Sit': 602, 'Work': 603, 'Wake': 604, 'Bend': 605, 'Walk': 606, 'Run': 607, 'First': 608, 'Second': 609, 'Third': 610, 'Time': 611, 'No': 612, 'Yes': 613, 'Now': 792, 'But': 615, 'Different': 616, 'Breathe': 617, 'Move': 618, 'Sometimes': 628, 'Often': 620, 'Constant': 621, 'Several': 622, 'Around': 623, 'Appear': 624, 'Relieve': 625, 'Comfort': 626, 'Eat': 994, 'Feel': 649, 'Food': 993, 'Down': 631, 'Help': 633, 'Take': 634, 'Stand': 636, 'Better': 757, 'Little': 638, 'Body': 641, 'Skin': 642, 'Stomach': 644, 'Where': 780, 'Allergy': 648, 'Medical': 651, 'History': 652, 'Calm': 653, 'Ambulance': 654, 'Reception': 655, 'Medical history': 656, 'Hello': 657, 'Good-bye': 658, 'Welcome': 659, 'Hearing': 660, 'Hard of hearing': 662, 'Family planning': 666, 'Vagina': 668, 'Monthly bleeding': 670, 'Love': 672, 'Please': 796, 'Smart': 674, 'Sorry': 675, 'Thank you': 771, 'Think': 677, 'Understand': 774, 'Bathroom': 755, 'Call': 680, 'Drink': 995, 'Water': 1013, 'More': 684, 'When': 685, 'Bad': 686, 'Good': 787, 'Breathing': 733, 'Hot': 690, 'Nurse': 692, 'Family': 856, 'Husband': 878, 'Wife': 879, 'Hungry': 996, 'Nausea': 743, 'Pain': 699, 'Tired': 773, 'Chills': 703, 'Numb': 704, 'Urinate': 705, 'Abuse': 706, 'Clean': 707, 'Wash': 951, 'Fever': 709, 'Pelvis': 710, 'Cramps': 712, 'Growth': 713, 'Depression': 719, 'Itchy': 720, 'Rape': 721, 'Blurred vision': 722, 'Labor': 724, 'Sleeping problems': 725, 'Discharge': 727, 'Sweating': 728, 'Breastfeeding': 729, 'Dizzy': 730, 'Swollen': 732, 'Drugs': 734, 'Miscarriage': 735, 'Tenderness': 736, 'Problems': 737, 'Dry': 738, 'Vomit': 775, 'Burn': 741, 'Medical examination': 742, 'Weak': 744, 'Changes in color': 745, 'Womb': 746, 'Contractions': 747, 'Exercise': 748, 'Cough': 768, 'Faint': 751, 'Pass by': 752, 'Stool': 753, 'Back': 754, 'Bed': 944, 'Breath': 759, 'Calm down': 760, 'Can': 761, 'Cannot': 762, 'Change': 763, 'Chest': 764, 'Ear infection': 765, \"Don't know\": 766, 'Write': 767, 'Dizziness': 769, 'Stay': 770, 'Thirst': 772, 'Wait': 776, 'Well': 779, 'Who': 781, 'Feeling': 783, 'Fine': 784, 'Friend': 897, 'Get up': 786, 'Hard': 789, 'Need': 791, 'Phone': 794, 'Pills': 795, 'Put on': 797, 'Sleepy': 799, 'Injection': 800, 'Interpret': 801, 'Improve': 802, 'Home': 930, 'Have': 805, 'Eleven': 806, 'Twelve': 807, 'Thirteen': 808, 'Fourteen': 809, 'Fifteen': 810, 'Sixteen': 811, 'Seventeen': 812, 'Eighteen': 813, 'Nineteen': 814, 'Twenty': 815, 'Twenty one': 816, 'Twenty two': 817, 'Twenty three': 818, 'Twenty four': 819, 'Twenty five': 820, 'Twenty six': 821, 'Twenty eight': 822, 'Twenty nine': 823, 'Thirty': 824, 'Thirty one': 825, 'Thirty two': 826, 'Thirty three': 827, 'Thirty four': 828, 'Thirty five': 829, 'Thirty six': 830, 'Thirty seven': 831, 'Thirty eight': 832, 'Thirty nine': 833, 'Forty': 834, 'Fifty': 835, 'Sixty': 836, 'Seventy': 837, 'Eighty': 838, 'Ninety': 839, 'Hundred': 840, 'Thousand': 841, 'One Million': 842, 'One Billion': 843, 'Colour': 844, 'Red': 845, 'Blue': 846, 'Yellow': 847, 'Green': 848, 'Orange': 849, 'Purple': 850, 'Violet': 851, 'Pink': 852, 'Brown': 853, 'Black': 854, 'White': 855, 'Father': 857, 'Mother': 858, 'Grandfather': 859, 'Grandmother': 860, 'Boy': 861, 'Girl': 862, 'Daughter': 863, 'Son': 864, 'Sister': 865, 'Brother': 866, 'Aunt': 869, 'Uncle': 868, 'Nephew': 870, 'Niece': 871, 'Cousin': 872, 'Relationship': 873, 'Sweatheart': 874, 'Engaged': 875, 'Marriage': 876, 'Wedding': 877, 'Baby': 880, 'Child': 881, 'Children': 882, 'Young': 883, 'Kid': 884, 'Twins': 885, 'In-law': 886, 'Divorced': 887, 'Adult': 888, 'Man': 889, 'Woman': 890, 'Gentleman': 891, 'Lady': 892, 'Person': 893, 'People': 894, 'Individual': 895, 'Name': 896, 'Enemy': 898, 'Group': 899, 'Neighbor': 900, 'Gang': 901, 'Robber': 902, 'Rasta': 903, 'White man': 904, 'My': 906, 'Mine': 907, 'Myself': 909, 'Yourself': 912, 'We': 913, 'Our': 915, 'He': 916, 'His': 917, 'Her': 918, 'Hers': 919, 'Someone': 920, 'Everyone': 921, 'Other': 922, 'This': 923, 'That': 924, 'There': 925, 'They': 926, 'Those': 927, 'Them': 928, 'These': 929, 'House': 931, 'Dormitory': 932, 'Roof': 933, 'Floor': 934, 'Wall': 935, 'Gate': 936, 'Room': 937, 'Door': 938, 'Window': 939, 'Glass': 940, 'Fan': 941, 'Table': 942, 'Chair': 943, 'Mat': 945, 'Mirror': 946, 'Refrigerator': 947, 'Basket': 948, 'Bucket': 949, 'Borehole': 950, 'Sponge': 953, 'Toilet': 954, 'Haircut': 955, 'Shave': 956, 'Iron': 957, 'Sewing': 958, 'Needle': 959, 'Tie knot': 960, 'Zip': 961, 'Soap': 962, 'Candle': 963, 'Thread': 964, 'Chain': 965, 'Blade': 966, 'Scissors': 967, 'Telephone': 968, 'Key': 969, 'Umbrella': 970, 'Wear': 971, 'Clothes': 972, 'Shirt': 973, 'Trousers': 974, 'Dress': 975, 'Skirt': 976, 'Coat': 977, 'Hat': 978, 'Gloves': 979, 'Necktie': 980, 'Socks': 981, 'Shoes': 982, 'Sandals': 983, 'Purse': 984, 'Bag': 985, 'Jewelry': 986, 'Cap': 987, 'Things': 988, 'Materials': 989, 'Breakfast': 990, 'Lunch': 991, 'Supper': 992, 'Starving': 997, 'Satisfied': 998, 'Bitter': 999, 'Sweet': 1000, 'Restaurant': 1001, 'Bowl': 1002, 'Plate': 1003, 'Cup': 1004, 'Spoon': 1005, 'Fork': 1006, 'Knife': 1007, 'Cook': 1008, 'Kitchen': 1009, 'Market': 1010, 'Coal pot': 1011, 'Grind': 1012, \"Hi, Doctor. I've been having some chest pains lately.\": 1014, \"Hello there. I'm sorry to hear that. \": 1015, 'Can you tell me more about your symptoms?': 1016, 'The pain is usually in the center of my chest. ': 1018, \"Okay, that's good information. \": 1019, 'Have you noticed any other symptoms.': 1020, \"Yes. I've been feeling a lightheaded and short of breath.\": 1021, 'I see. It sounds like we should do some tests.': 1022, 'That sounds a bit scary. Is it serious? ': 1024, \"It's too early to say.\": 1026, 'I understand. What else can I do to?': 1028, 'If you smoke or drink, now is a good time to quit. ': 1030, 'Try to eat a healthy diet and get some exercise if you can': 1031, \"If your symptoms worsen, don't hesitate to contact me.\": 1033, 'Thank you, Doctor. I appreciate your help.': 1034, \"Take care and I'll be in touch as soon as we have the results.\": 1036, \"Good morning, doctor. I've been feeling really tired lately \": 1038, \"Good morning. Let's start by getting some more information\": 1039, 'When did you first start feeling tired?': 1040, \"I've been sleeping more than usual\": 1042, 'I still feel exhausted.': 1044, 'Have you been experiencing any other symptoms?': 1046, 'No, just some headaches.': 1048, \"Okay, I'd like to do some tests \": 1050, 'Okay. Is there anything I should be doing in the meantime': 1052, \"Yes, make sure you're getting plenty of rest and staying hydrated. \": 1054, 'Try to eat a balanced diet and get some light exercise as well.': 1056, \"Okay, I'll do my best. Thanks, doctor.\": 1058}\n"
     ]
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(y)}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1059,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_508\\2745492590.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train = np.array(padded_arr)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(padded_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1059, 372)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1662"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[6][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (2,2)  and requested shape (1,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m padded_arrays\n\u001b[0;32m     14\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# Assuming array_list is your list containing the 2D arrays\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m padded_list \u001b[39m=\u001b[39m pad_2d_arrays(X)\n",
      "Cell \u001b[1;32mIn[198], line 9\u001b[0m, in \u001b[0;36mpad_2d_arrays\u001b[1;34m(array_list)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m array_list:\n\u001b[0;32m      8\u001b[0m     pad_width \u001b[39m=\u001b[39m ((\u001b[39m0\u001b[39m, max_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(arr)), (\u001b[39m0\u001b[39m, \u001b[39m1662\u001b[39m))  \u001b[39m# Calculate the padding width\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     padded_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mpad(arr, pad_width, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mconstant\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# Pad the array\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     padded_arrays\u001b[39m.\u001b[39mappend(padded_arr)\n\u001b[0;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m padded_arrays\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraypad.py:743\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`pad_width` must be of integral type.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    742\u001b[0m \u001b[39m# Broadcast to shape (array.ndim, 2)\u001b[39;00m\n\u001b[1;32m--> 743\u001b[0m pad_width \u001b[39m=\u001b[39m _as_pairs(pad_width, array\u001b[39m.\u001b[39;49mndim, as_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    745\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(mode):\n\u001b[0;32m    746\u001b[0m     \u001b[39m# Old behavior: Use user-supplied function with np.apply_along_axis\u001b[39;00m\n\u001b[0;32m    747\u001b[0m     function \u001b[39m=\u001b[39m mode\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraypad.py:518\u001b[0m, in \u001b[0;36m_as_pairs\u001b[1;34m(x, ndim, as_index)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindex can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain negative values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    516\u001b[0m \u001b[39m# Converting the array with `tolist` seems to improve performance\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m# when iterating and indexing the result (see usage in `pad`)\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mbroadcast_to(x, (ndim, \u001b[39m2\u001b[39;49m))\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:413\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[1;34m(array, shape, subok)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_broadcast_to_dispatcher, module\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbroadcast_to\u001b[39m(array, shape, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    369\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[0;32m    371\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39m           [1, 2, 3]])\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m _broadcast_to(array, shape, subok\u001b[39m=\u001b[39;49msubok, readonly\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:349\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[1;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall elements of broadcast shape must be non-\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    347\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    348\u001b[0m extras \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 349\u001b[0m it \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mnditer(\n\u001b[0;32m    350\u001b[0m     (array,), flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmulti_index\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrefs_ok\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mzerosize_ok\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m+\u001b[39;49m extras,\n\u001b[0;32m    351\u001b[0m     op_flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mreadonly\u001b[39;49m\u001b[39m'\u001b[39;49m], itershape\u001b[39m=\u001b[39;49mshape, order\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    352\u001b[0m \u001b[39mwith\u001b[39;00m it:\n\u001b[0;32m    353\u001b[0m     \u001b[39m# never really has writebackifcopy semantics\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     broadcast \u001b[39m=\u001b[39m it\u001b[39m.\u001b[39mitviews[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (2,2)  and requested shape (1,2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "padded_arrays = []\n",
    "def pad_2d_arrays(array_list):\n",
    "    max_length = max(len(arr) for arr in array_list)  # Find the maximum length of 2D arrays\n",
    "\n",
    "    \n",
    "    for arr in array_list:\n",
    "        pad_width = ((0, max_length - len(arr)), (0, 1662))  # Calculate the padding width\n",
    "        padded_arr = np.pad(arr, pad_width, mode='constant')  # Pad the array\n",
    "        padded_arrays.append(padded_arr)\n",
    "\n",
    "    return padded_arrays\n",
    "\n",
    "# Example usage\n",
    "# Assuming array_list is your list containing the 2D arrays\n",
    "padded_list = pad_2d_arrays(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Reshape the word list to a 2D array (n_samples, 1)\n",
    "word_list_2d = [[word] for word in y]\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot_encoded = encoder.fit_transform(word_list_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_508\\1045748640.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  model.fit(np.array(X), y, epochs=2000, callbacks=[tb_callback])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[181], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(X), y, epochs\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[tb_callback])\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1082\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1079\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1081\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1082\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1083\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1084\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1085\u001b[0m         )\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})"
     ]
    }
   ],
   "source": [
    "model.fit(np.array(X), y, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_508\\2318462743.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(X).dtype)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(X).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def check_nested_array(nested_array):\n",
    "    # Check the structure of the nested array\n",
    "    nested_array_length = len(nested_array[0])  # Assuming all nested arrays have the same length\n",
    "    for nested in nested_array:\n",
    "        if len(nested) != nested_array_length:\n",
    "            print(len(nested))\n",
    "            return False\n",
    "    \n",
    "    # Check the data types within the nested array\n",
    "    expected_type = type(nested_array[0][0])  # Assuming all elements have the same type\n",
    "    for nested in nested_array:\n",
    "        for element in nested:\n",
    "            print(type(element))\n",
    "            if type(element) != expected_type:\n",
    "                print(type(element))\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "is_valid = check_nested_array(padded_arr)\n",
    "print(is_valid)  # Output: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.50286561,  0.24282664, -0.52643996, ...,  0.52174175,\n",
       "         0.91284168, -0.04412461]),\n",
       " array([ 0.49220952,  0.26368633, -0.52138311, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.48695838,  0.26897791, -0.50165528, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.48648584,  0.26829833, -0.46982089, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.48951045,  0.26551896, -0.46566802, ...,  0.51998889,\n",
       "         0.75455689, -0.0337056 ]),\n",
       " array([ 0.49124673,  0.26485133, -0.45838943, ...,  0.52111924,\n",
       "         0.72124308, -0.03975023]),\n",
       " array([ 0.49239612,  0.26692066, -0.42371807, ...,  0.52017283,\n",
       "         0.69272232, -0.03785754]),\n",
       " array([ 0.49176252,  0.2659457 , -0.40783477, ...,  0.51705527,\n",
       "         0.66946453, -0.03609408]),\n",
       " array([ 0.49232879,  0.26530331, -0.41583815, ...,  0.51378584,\n",
       "         0.65177101, -0.03582725]),\n",
       " array([ 0.48999047,  0.26471007, -0.41455132, ...,  0.51432335,\n",
       "         0.6380294 , -0.03463049]),\n",
       " array([ 0.48994496,  0.2642093 , -0.40592614, ...,  0.51457918,\n",
       "         0.62381035, -0.03644062]),\n",
       " array([ 0.48852977,  0.26479253, -0.43017522, ...,  0.51668316,\n",
       "         0.61259669, -0.03939164]),\n",
       " array([ 0.48844093,  0.26449344, -0.44166479, ...,  0.51814711,\n",
       "         0.60111845, -0.04095148]),\n",
       " array([ 0.48822004,  0.26357958, -0.42260551, ...,  0.52030909,\n",
       "         0.5949465 , -0.03961254]),\n",
       " array([ 0.48715633,  0.26088372, -0.40024194, ...,  0.52155316,\n",
       "         0.58985734, -0.04441011]),\n",
       " array([ 0.48730373,  0.26143742, -0.36018649, ...,  0.52265835,\n",
       "         0.58591789, -0.03916302]),\n",
       " array([ 0.48734969,  0.26197001, -0.35248756, ...,  0.521357  ,\n",
       "         0.58555073, -0.04147872]),\n",
       " array([ 0.48742759,  0.26516569, -0.35527024, ...,  0.52183658,\n",
       "         0.58660173, -0.04468952]),\n",
       " array([ 0.48794138,  0.2662141 , -0.36724517, ...,  0.52061439,\n",
       "         0.58782125, -0.04451484]),\n",
       " array([ 0.4881067 ,  0.26763907, -0.37712196, ...,  0.52291751,\n",
       "         0.59346139, -0.04036581]),\n",
       " array([ 0.48836818,  0.26774472, -0.38086951, ...,  0.52593488,\n",
       "         0.59611285, -0.03959511]),\n",
       " array([ 0.48884156,  0.26758602, -0.3848595 , ...,  0.52477324,\n",
       "         0.60037702, -0.04092208]),\n",
       " array([ 0.48889908,  0.26702544, -0.49950421, ...,  0.52784204,\n",
       "         0.60344368, -0.03743892]),\n",
       " array([ 0.48891807,  0.26768085, -0.48729447, ...,  0.52805591,\n",
       "         0.60762423, -0.03460733]),\n",
       " array([ 0.48886716,  0.26764914, -0.47029796, ...,  0.52864873,\n",
       "         0.61345923, -0.03368434]),\n",
       " array([ 0.48891747,  0.26815897, -0.48279446, ...,  0.5262723 ,\n",
       "         0.61974895, -0.03467602]),\n",
       " array([ 0.48891708,  0.26845089, -0.4804799 , ...,  0.52728051,\n",
       "         0.62914586, -0.03306761]),\n",
       " array([ 0.48919851,  0.26834947, -0.47184947, ...,  0.52566075,\n",
       "         0.63377392, -0.03343812]),\n",
       " array([ 0.48949748,  0.26830995, -0.4670707 , ...,  0.52502549,\n",
       "         0.64144087, -0.03051178]),\n",
       " array([ 0.48941717,  0.26829064, -0.47784632, ...,  0.51883292,\n",
       "         0.65119767, -0.02928502]),\n",
       " array([ 0.48976594,  0.26718965, -0.47609696, ...,  0.51856112,\n",
       "         0.65954721, -0.02945811]),\n",
       " array([ 0.48983902,  0.26730284, -0.47330582, ...,  0.51817811,\n",
       "         0.66799432, -0.02564378]),\n",
       " array([ 0.4902823 ,  0.26735827, -0.52154493, ...,  0.51669288,\n",
       "         0.67648929, -0.02774048]),\n",
       " array([ 0.49029636,  0.2677016 , -0.52661711, ...,  0.51441044,\n",
       "         0.67941737, -0.03132696]),\n",
       " array([ 0.49029389,  0.26712057, -0.51776171, ...,  0.51132399,\n",
       "         0.68081868, -0.03225373]),\n",
       " array([ 0.49032918,  0.26445773, -0.54681039, ...,  0.51230568,\n",
       "         0.67936867, -0.03596767]),\n",
       " array([ 0.49057105,  0.26414746, -0.55459726, ...,  0.51204044,\n",
       "         0.67609859, -0.04006481]),\n",
       " array([ 0.4906325 ,  0.26138303, -0.54286367, ...,  0.51110679,\n",
       "         0.67482573, -0.03772407]),\n",
       " array([ 0.49088043,  0.25782302, -0.53765213, ...,  0.51021862,\n",
       "         0.67285955, -0.04197389]),\n",
       " array([ 0.49077234,  0.25539434, -0.53360903, ...,  0.50874007,\n",
       "         0.67051667, -0.03880079]),\n",
       " array([ 0.49069178,  0.25396183, -0.54354489, ...,  0.50977904,\n",
       "         0.67095315, -0.03715565]),\n",
       " array([ 0.49069428,  0.25368953, -0.55027366, ...,  0.51178318,\n",
       "         0.67210221, -0.03878614]),\n",
       " array([ 0.49068782,  0.2521548 , -0.56562638, ...,  0.51086289,\n",
       "         0.67406404, -0.03593095]),\n",
       " array([ 0.49167034,  0.25147107, -0.57384717, ...,  0.51153934,\n",
       "         0.67794681, -0.03627169]),\n",
       " array([ 0.49177161,  0.24816196, -0.56666213, ...,  0.51129532,\n",
       "         0.6782099 , -0.03817024]),\n",
       " array([ 0.49182016,  0.24810806, -0.57058656, ...,  0.51215732,\n",
       "         0.6778211 , -0.04051951]),\n",
       " array([ 0.49258393,  0.24821329, -0.56786019, ...,  0.51297027,\n",
       "         0.68213743, -0.03965223]),\n",
       " array([ 0.4926241 ,  0.24828219, -0.55166852, ...,  0.51971871,\n",
       "         0.68899769, -0.04046725]),\n",
       " array([ 0.49284047,  0.24834663, -0.55140519, ...,  0.52578044,\n",
       "         0.70688093, -0.02798592]),\n",
       " array([ 0.49267715,  0.24721992, -0.52080923, ...,  0.53470546,\n",
       "         0.72755075, -0.0308293 ]),\n",
       " array([ 0.49331763,  0.24923824, -0.46283558, ...,  0.53700584,\n",
       "         0.74137217, -0.03909526]),\n",
       " array([ 0.49370012,  0.24970947, -0.45397162, ...,  0.53902882,\n",
       "         0.75568789, -0.04722588]),\n",
       " array([ 0.49385107,  0.25054967, -0.4542402 , ...,  0.54071367,\n",
       "         0.7590518 , -0.05706033]),\n",
       " array([ 0.49430659,  0.25160947, -0.4530836 , ...,  0.54347384,\n",
       "         0.7654233 , -0.06049604]),\n",
       " array([ 0.49426475,  0.25281969, -0.45746964, ...,  0.54398102,\n",
       "         0.76528931, -0.0627111 ]),\n",
       " array([ 0.49426681,  0.25363964, -0.47148842, ...,  0.54809493,\n",
       "         0.76250482, -0.0614121 ]),\n",
       " array([ 0.49358904,  0.25232387, -0.4760991 , ...,  0.55095696,\n",
       "         0.75006056, -0.06282569]),\n",
       " array([ 0.49381596,  0.25240594, -0.51594073, ...,  0.55444121,\n",
       "         0.74232686, -0.06240462]),\n",
       " array([ 0.49396256,  0.25248739, -0.50752062, ...,  0.56078708,\n",
       "         0.74285978, -0.06676839]),\n",
       " array([ 0.49377826,  0.2524952 , -0.48860756, ...,  0.56107068,\n",
       "         0.74015427, -0.064096  ]),\n",
       " array([ 0.49380878,  0.2529375 , -0.4923293 , ...,  0.56435138,\n",
       "         0.73736787, -0.06536952]),\n",
       " array([ 0.49526268,  0.25273052, -0.55155307, ...,  0.57059026,\n",
       "         0.74801433, -0.0604864 ]),\n",
       " array([ 0.4960207 ,  0.25270206, -0.54816133, ...,  0.56921709,\n",
       "         0.7545439 , -0.06552035]),\n",
       " array([ 0.49639893,  0.25286013, -0.50025773, ...,  0.57048595,\n",
       "         0.75078017, -0.06347004]),\n",
       " array([ 0.49731049,  0.25288779, -0.53049421, ...,  0.5702883 ,\n",
       "         0.74614227, -0.06393344]),\n",
       " array([ 0.49752015,  0.25305802, -0.57290608, ...,  0.57179993,\n",
       "         0.7403273 , -0.06763154]),\n",
       " array([ 0.49751899,  0.253672  , -0.56932467, ...,  0.57149529,\n",
       "         0.74600714, -0.05962027]),\n",
       " array([ 0.49756381,  0.25481528, -0.56003106, ...,  0.57174754,\n",
       "         0.73977351, -0.0558197 ]),\n",
       " array([ 0.4978978 ,  0.25546974, -0.55425477, ...,  0.56989098,\n",
       "         0.71830761, -0.05120908]),\n",
       " array([ 0.49785084,  0.25624612, -0.53311193, ...,  0.56067115,\n",
       "         0.69691044, -0.04323442]),\n",
       " array([ 0.49785846,  0.25693816, -0.53405714, ...,  0.5225929 ,\n",
       "         0.64597124, -0.03169343]),\n",
       " array([ 0.49789357,  0.25958347, -0.49361503, ...,  0.46583444,\n",
       "         0.6352793 , -0.03239816]),\n",
       " array([ 0.49842462,  0.26287681, -0.45201543, ...,  0.43640494,\n",
       "         0.62264109, -0.03319005]),\n",
       " array([ 0.49843007,  0.26368442, -0.4647429 , ...,  0.41749844,\n",
       "         0.61898506, -0.03798899]),\n",
       " array([ 0.49785352,  0.26365143, -0.46272182, ...,  0.407803  ,\n",
       "         0.61788923, -0.03939207]),\n",
       " array([ 0.49781191,  0.26460713, -0.45015562, ...,  0.41377437,\n",
       "         0.61364108, -0.03861213]),\n",
       " array([ 0.49860653,  0.26548642, -0.45388016, ...,  0.42866081,\n",
       "         0.61194932, -0.03949456]),\n",
       " array([ 0.50046378,  0.26705143, -0.45614338, ...,  0.45150003,\n",
       "         0.61416763, -0.03578043]),\n",
       " array([ 0.50119257,  0.26756573, -0.47339439, ...,  0.47239617,\n",
       "         0.62186885, -0.03700746]),\n",
       " array([ 0.50160056,  0.26777908, -0.47515923, ...,  0.47669038,\n",
       "         0.6212433 , -0.03131057]),\n",
       " array([ 0.50186884,  0.26886261, -0.50637782, ...,  0.46679226,\n",
       "         0.61157537, -0.03061223]),\n",
       " array([ 0.50300986,  0.27059865, -0.49752429, ...,  0.45049727,\n",
       "         0.61168504, -0.03545896]),\n",
       " array([ 0.5030058 ,  0.27379254, -0.47978932, ...,  0.44020146,\n",
       "         0.61848062, -0.03989222]),\n",
       " array([ 0.50291365,  0.27486843, -0.47988072, ...,  0.44121566,\n",
       "         0.62495452, -0.03882124]),\n",
       " array([ 0.50309336,  0.27482751, -0.50518703, ...,  0.45184737,\n",
       "         0.63158727, -0.03590808]),\n",
       " array([ 0.5036431 ,  0.27527004, -0.50317013, ...,  0.4558531 ,\n",
       "         0.63431025, -0.03540577]),\n",
       " array([ 0.50382173,  0.27532762, -0.4981426 , ...,  0.45756841,\n",
       "         0.63400602, -0.03057628]),\n",
       " array([ 0.50434911,  0.27534455, -0.48110947, ...,  0.49066103,\n",
       "         0.63768482, -0.03912008]),\n",
       " array([ 0.50481528,  0.27498335, -0.47622603, ...,  0.50747502,\n",
       "         0.63035578, -0.04957717]),\n",
       " array([ 0.50524193,  0.2738764 , -0.46136844, ...,  0.530707  ,\n",
       "         0.60761362, -0.04766876]),\n",
       " array([ 0.50524348,  0.27375457, -0.44459012, ...,  0.5397253 ,\n",
       "         0.60350066, -0.05336345]),\n",
       " array([ 0.50564349,  0.27174512, -0.49843326, ...,  0.55339319,\n",
       "         0.59660208, -0.05580446]),\n",
       " array([ 0.50577581,  0.27066204, -0.50379395, ...,  0.56948787,\n",
       "         0.59109312, -0.06383712]),\n",
       " array([ 0.50605494,  0.27011904, -0.50405884, ...,  0.57857174,\n",
       "         0.58506811, -0.05300621]),\n",
       " array([ 0.50605905,  0.2695691 , -0.49526197, ...,  0.59723318,\n",
       "         0.57749814, -0.04870913]),\n",
       " array([ 0.50603801,  0.27116027, -0.49595693, ...,  0.60519719,\n",
       "         0.57352448, -0.04162116]),\n",
       " array([ 0.50623775,  0.27246532, -0.48288545, ...,  0.60574591,\n",
       "         0.57789016, -0.05839422]),\n",
       " array([ 0.50626838,  0.27106836, -0.48826051, ...,  0.59259003,\n",
       "         0.54179734, -0.04255831]),\n",
       " array([ 0.50628871,  0.26842314, -0.47239599, ...,  0.56871092,\n",
       "         0.5219568 , -0.04883599]),\n",
       " array([ 0.50654787,  0.26869509, -0.47268033, ...,  0.56819183,\n",
       "         0.51247567, -0.07418593]),\n",
       " array([ 0.50770408,  0.27224332, -0.4473246 , ...,  0.57202828,\n",
       "         0.53574157, -0.06964069]),\n",
       " array([ 0.5080049 ,  0.27782863, -0.45293954, ...,  0.55866593,\n",
       "         0.5710516 , -0.06715025]),\n",
       " array([ 0.50788057,  0.27811971, -0.45524415, ...,  0.5565477 ,\n",
       "         0.58840191, -0.05867797]),\n",
       " array([ 0.50816745,  0.28014624, -0.48522514, ...,  0.55535877,\n",
       "         0.59714252, -0.04399236]),\n",
       " array([ 0.50825709,  0.27949306, -0.50974673, ...,  0.56080949,\n",
       "         0.61525297, -0.06486171]),\n",
       " array([ 0.50824213,  0.2767897 , -0.54199106, ...,  0.54813129,\n",
       "         0.62816918, -0.04305071]),\n",
       " array([ 0.50824553,  0.27673748, -0.56436396, ...,  0.54069674,\n",
       "         0.63052082, -0.04972328]),\n",
       " array([ 0.50898081,  0.27760762, -0.60674304, ...,  0.56694156,\n",
       "         0.57059705, -0.03501999]),\n",
       " array([ 0.50915831,  0.27910954, -0.69513351, ...,  0.57355279,\n",
       "         0.56642276, -0.03529144]),\n",
       " array([ 0.50947404,  0.28047007, -0.70463365, ...,  0.51839143,\n",
       "         0.64533973, -0.03885463]),\n",
       " array([ 0.51027071,  0.28012609, -0.65721834, ...,  0.48989183,\n",
       "         0.67120439, -0.03282905]),\n",
       " array([ 0.51002502,  0.2800853 , -0.64135015, ...,  0.4611671 ,\n",
       "         0.67961401, -0.04317597]),\n",
       " array([ 0.5100767 ,  0.27664837, -0.61195558, ...,  0.44093722,\n",
       "         0.66974455, -0.0276239 ]),\n",
       " array([ 0.51007891,  0.27585945, -0.61028898, ...,  0.43386182,\n",
       "         0.67351693, -0.02761051]),\n",
       " array([ 0.51007259,  0.27478054, -0.57086813, ...,  0.43288705,\n",
       "         0.67556679, -0.03306793]),\n",
       " array([ 0.51006478,  0.27478272, -0.74043632, ...,  0.44182813,\n",
       "         0.68196219, -0.03476254]),\n",
       " array([ 0.50991893,  0.27497515, -0.60558558, ...,  0.44332516,\n",
       "         0.6694802 , -0.02572278]),\n",
       " array([ 0.51037675,  0.27876472, -0.68302816, ...,  0.45612341,\n",
       "         0.6871478 , -0.02719439]),\n",
       " array([ 0.51049429,  0.2827552 , -0.73145109, ...,  0.47290635,\n",
       "         0.67677778, -0.03068117]),\n",
       " array([ 0.51045573,  0.28326267, -0.70642251, ...,  0.49801916,\n",
       "         0.69223309, -0.03213074]),\n",
       " array([ 0.51027101,  0.28773984, -0.74684644, ...,  0.50719088,\n",
       "         0.69366664, -0.04144498]),\n",
       " array([ 0.50981992,  0.28754809, -0.74905199, ...,  0.5008021 ,\n",
       "         0.69684184, -0.03709566]),\n",
       " array([ 0.50968844,  0.2874606 , -0.7023192 , ...,  0.49166223,\n",
       "         0.70412332, -0.0501541 ]),\n",
       " array([ 0.50971854,  0.28349885, -0.68209708, ...,  0.48739475,\n",
       "         0.71609843, -0.06970871]),\n",
       " array([ 0.51039654,  0.28347802, -0.62980819, ...,  0.51919675,\n",
       "         0.62472641, -0.04681268]),\n",
       " array([ 0.51068997,  0.2833201 , -0.63668573, ...,  0.50483507,\n",
       "         0.64887911, -0.04804717]),\n",
       " array([ 0.51139742,  0.28226921, -0.64214879, ...,  0.51561558,\n",
       "         0.63200766, -0.05220798]),\n",
       " array([ 0.51226377,  0.28129414, -0.63258421, ...,  0.53038055,\n",
       "         0.61987168, -0.08433496]),\n",
       " array([ 0.51360536,  0.2803652 , -0.59240812, ...,  0.54591179,\n",
       "         0.62770593, -0.06268053]),\n",
       " array([ 0.51373297,  0.28005061, -0.59307313, ...,  0.56371009,\n",
       "         0.62426376, -0.05422341]),\n",
       " array([ 0.5140413 ,  0.27883443, -0.60419363, ...,  0.57234925,\n",
       "         0.6257931 , -0.05486523]),\n",
       " array([ 0.51424819,  0.27861038, -0.62225813, ...,  0.57433385,\n",
       "         0.62559551, -0.0598428 ]),\n",
       " array([ 0.51402634,  0.27835628, -0.57450593, ...,  0.57412469,\n",
       "         0.63583899, -0.05692325]),\n",
       " array([ 0.51399797,  0.27767023, -0.60868001, ...,  0.57433909,\n",
       "         0.64140952, -0.06165574]),\n",
       " array([ 0.51393557,  0.2736648 , -0.60333794, ...,  0.57554162,\n",
       "         0.64856637, -0.06245983]),\n",
       " array([ 0.51425987,  0.26530901, -0.61601943, ...,  0.57814801,\n",
       "         0.65300679, -0.05784723]),\n",
       " array([ 0.51446128,  0.26141039, -0.65290558, ...,  0.5728339 ,\n",
       "         0.6612668 , -0.06700038]),\n",
       " array([ 0.51479268,  0.25448111, -0.671673  , ...,  0.57144445,\n",
       "         0.67029065, -0.0604974 ]),\n",
       " array([ 0.51479566,  0.25212801, -0.72588956, ...,  0.56948763,\n",
       "         0.69908065, -0.05096473]),\n",
       " array([ 0.51471621,  0.24880864, -0.77086663, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.51376778,  0.24434364, -0.82009745, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.5140416,  0.2353652, -0.8888191, ...,  0.       ,  0.       ,\n",
       "         0.       ]),\n",
       " array([ 0.51413357,  0.23024012, -0.83480859, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.51413757,  0.22208948, -0.81766015, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.51409197,  0.21819514, -0.67856431, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.51372862,  0.21343747, -0.59909421, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.51390767,  0.20980224, -0.61511183, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.5138942 ,  0.21010374, -0.61806124, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.51534557,  0.20991328, -0.53565389, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.51559877,  0.20763463, -0.53571337, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.5155533 ,  0.20662586, -0.5399456 , ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.51637828,  0.20712331, -0.56946743, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_arr[-650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_int_subarrays(nested_array):\n",
    "    count = 0\n",
    "    for subarray in nested_array:\n",
    "        if any(isinstance(element, int) for element in subarray):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Example usage\n",
    "nested_array = padded_arr\n",
    "int_subarray_count = count_int_subarrays(nested_array)\n",
    "print(int_subarray_count)  # Output: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4571802 ,  0.22804336, -0.527282  , ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_arr[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in padded_arr:\n",
    "    for y in \n",
    "    if len(i) != 372:\n",
    "        print(False)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m X_processed \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m X:\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(sublist[\u001b[39m0\u001b[39;49m], \u001b[39mlist\u001b[39m):\n\u001b[0;32m      4\u001b[0m         X_processed\u001b[39m.\u001b[39mextend(sublist)\n\u001b[0;32m      5\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "X_processed = []\n",
    "for sublist in X:\n",
    "    if isinstance(sublist[0], list):\n",
    "        X_processed.extend(sublist)\n",
    "    else:\n",
    "        X_processed.append(sublist)\n",
    "X_processed = np.array(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_processed, (X_processed.shape[0], 1, X_processed.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})', '(<class \\'list\\'> containing values of types {\"<class \\'numpy.ndarray\\'>\"})', '(<class \\'list\\'> containing values of types {\"<class \\'numpy.ndarray\\'>\", \"<class \\'int\\'>\"})'}), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[214], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39madd(Dense(num_classes, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mfit(padded_arr, y_encoded, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1082\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1079\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1081\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1082\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1083\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1084\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1085\u001b[0m         )\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})', '(<class \\'list\\'> containing values of types {\"<class \\'numpy.ndarray\\'>\"})', '(<class \\'list\\'> containing values of types {\"<class \\'numpy.ndarray\\'>\", \"<class \\'int\\'>\"})'}), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(padded_arr, y_encoded, epochs=10, batch_size=32,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_508\\1273227418.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(padded_arr).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1059, 372)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded_arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.load(\"X.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071, 618264)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[34][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"X.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45600423,  0.2333079 , -0.506378  , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.47096324,  0.23414364, -0.81818223, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.46856007,  0.21675549, -0.70816606, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.49104264,  0.27044585, -0.60848659, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.48748872,  0.28047171, -0.56018519, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.51354039,  0.24837087, -0.83206671, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.56004232e-01,  2.33307898e-01, -5.06377995e-01,  9.99777853e-01,\n",
       "        4.75985676e-01,  1.90068424e-01, -4.78294104e-01,  9.99634504e-01,\n",
       "        4.89453167e-01,  1.89619482e-01, -4.78280246e-01,  9.99605238e-01,\n",
       "        5.00471950e-01,  1.89701259e-01, -4.78520215e-01,  9.99657869e-01,\n",
       "        4.34933573e-01,  1.94741786e-01, -4.77371871e-01,  9.99725521e-01,\n",
       "        4.23198521e-01,  1.97234690e-01, -4.77218747e-01,  9.99715626e-01,\n",
       "        4.14740801e-01,  2.00044155e-01, -4.77521986e-01,  9.99781072e-01,\n",
       "        5.21495759e-01,  2.15462804e-01, -2.81138867e-01,  9.99603331e-01,\n",
       "        4.07695651e-01,  2.25192621e-01, -2.63448089e-01,  9.99781907e-01,\n",
       "        4.89424437e-01,  2.78147221e-01, -4.31510329e-01,  9.99895573e-01,\n",
       "        4.39041227e-01,  2.79931903e-01, -4.26113307e-01,  9.99925017e-01,\n",
       "        6.26905918e-01,  4.99994755e-01, -1.98663011e-01,  9.99234319e-01,\n",
       "        3.38460505e-01,  5.08986771e-01, -1.92352444e-01,  9.99715507e-01,\n",
       "        6.60757363e-01,  8.09577167e-01, -1.41313776e-01,  8.68441641e-01,\n",
       "        3.05986226e-01,  8.11105132e-01, -1.31488681e-01,  8.53444695e-01,\n",
       "        6.29363775e-01,  1.00580847e+00, -2.71018773e-01,  1.66407704e-01,\n",
       "        2.98675358e-01,  1.05661798e+00, -3.16090852e-01,  2.40749002e-01,\n",
       "        6.50310695e-01,  1.10309231e+00, -3.26103181e-01,  1.33836925e-01,\n",
       "        2.91207850e-01,  1.13987768e+00, -3.70159805e-01,  1.92648649e-01,\n",
       "        6.25999868e-01,  1.08845639e+00, -3.64097327e-01,  1.73462003e-01,\n",
       "        3.09358925e-01,  1.12380934e+00, -4.26250845e-01,  2.36336738e-01,\n",
       "        6.18420780e-01,  1.05457580e+00, -2.90854037e-01,  1.98525414e-01,\n",
       "        3.21339697e-01,  1.09084952e+00, -3.45197946e-01,  2.65793532e-01,\n",
       "        5.75740993e-01,  1.03524816e+00, -8.21938086e-03,  5.44710994e-01,\n",
       "        4.00978595e-01,  1.02815175e+00,  9.97023936e-03,  5.61884642e-01,\n",
       "        5.71955860e-01,  1.44728374e+00,  8.45307857e-02,  1.51199950e-02,\n",
       "        3.87231231e-01,  1.44835043e+00,  1.03656232e-01,  9.03910026e-03,\n",
       "        5.67765594e-01,  1.79283464e+00,  5.02276540e-01,  1.15059805e-03,\n",
       "        3.91411275e-01,  1.79156029e+00,  4.01470780e-01,  6.06993970e-04,\n",
       "        5.73268890e-01,  1.84536374e+00,  5.29104769e-01,  1.18097698e-03,\n",
       "        3.90227914e-01,  1.84524572e+00,  4.22955632e-01,  9.99787822e-04,\n",
       "        5.42449296e-01,  1.92097592e+00,  2.39132449e-01,  6.87353779e-04,\n",
       "        4.09124643e-01,  1.91977251e+00,  9.86383855e-02,  6.96827075e-04,\n",
       "        4.65274811e-01,  2.81435728e-01, -1.69907846e-02,  4.62782383e-01,\n",
       "        2.46254459e-01, -3.00604645e-02,  4.63442534e-01,  2.56376058e-01,\n",
       "       -1.62976645e-02,  4.55984384e-01,  2.14196309e-01, -2.19164230e-02,\n",
       "        4.62244898e-01,  2.36330256e-01, -3.17773819e-02,  4.61473376e-01,\n",
       "        2.23135307e-01, -2.92668976e-02,  4.59843248e-01,  1.90543130e-01,\n",
       "       -1.29489098e-02,  4.18691248e-01,  1.93387285e-01,  7.80414045e-03,\n",
       "        4.58332896e-01,  1.63136765e-01, -8.14513303e-03,  4.57590908e-01,\n",
       "        1.49407834e-01, -8.68884288e-03,  4.54647362e-01,  1.07888564e-01,\n",
       "       -1.18392671e-03,  4.65633124e-01,  2.86331058e-01, -1.62134767e-02,\n",
       "        4.65939432e-01,  2.90205121e-01, -1.43237105e-02,  4.66124505e-01,\n",
       "        2.91876346e-01, -1.18275275e-02,  4.66275990e-01,  2.96653748e-01,\n",
       "       -1.06529109e-02,  4.66571540e-01,  3.00947547e-01, -1.16336923e-02,\n",
       "        4.66917723e-01,  3.06191742e-01, -1.30411955e-02,  4.67313319e-01,\n",
       "        3.11140060e-01, -1.25636775e-02,  4.67825741e-01,  3.16500545e-01,\n",
       "       -8.07247683e-03,  4.63050336e-01,  2.51239568e-01, -2.72847991e-02,\n",
       "        4.57140595e-01,  2.51453847e-01, -1.97322071e-02,  3.94587845e-01,\n",
       "        1.58866480e-01,  3.55857834e-02,  4.36005354e-01,  2.00475216e-01,\n",
       "        1.98514597e-03,  4.30233568e-01,  2.02032894e-01,  2.31579295e-03,\n",
       "        4.24574643e-01,  2.02110082e-01,  3.82151594e-03,  4.16979730e-01,\n",
       "        1.96756929e-01,  8.54409020e-03,  4.40672487e-01,  1.97140425e-01,\n",
       "        2.87820119e-03,  4.25948888e-01,  1.72081292e-01,  5.29228826e-04,\n",
       "        4.32655364e-01,  1.72182500e-01,  7.44532270e-04,  4.20030087e-01,\n",
       "        1.74823269e-01,  2.20921915e-03,  4.16377425e-01,  1.79555967e-01,\n",
       "        4.35298029e-03,  4.12726313e-01,  2.04003990e-01,  1.12129897e-02,\n",
       "        4.48161304e-01,  3.27981204e-01, -1.76185230e-03,  4.16411847e-01,\n",
       "        1.90592140e-01,  9.67308693e-03,  3.94921780e-01,  2.00058907e-01,\n",
       "        3.61029170e-02,  4.05629218e-01,  1.97165459e-01,  1.70564372e-02,\n",
       "        4.34015274e-01,  2.41309673e-01, -4.39635664e-03,  4.58083600e-01,\n",
       "        2.80875236e-01, -1.63358003e-02,  4.60229963e-01,  2.90990144e-01,\n",
       "       -1.35339219e-02,  4.51288044e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][: 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = X.reshape(X.shape[0], 372, 1662)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "new_y = np.load(\"Y.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071, 372, 1662)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_x = new_x[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071, 372, 1662)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1662"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_x[0][-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_y = Y[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "code = np.array(new_y)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "vec = label_encoder.fit_transform(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, BatchNormalization\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, activation='relu', input_shape=(372,1662)))\n",
    "# model.add(BatchNormalization())\n",
    "# # model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# # model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(189, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 372, 1662)]       0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 372, 512)          851456    \n",
      "                                                                 \n",
      " layer_normalization_10 (Lay  (None, 372, 512)         1024      \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 372, 512)          0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 372, 512)          0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 372, 256)          131328    \n",
      "                                                                 \n",
      " layer_normalization_11 (Lay  (None, 372, 256)         512       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 372, 256)          0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 372, 256)          0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 250)               507000    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 878)               220378    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,711,698\n",
      "Trainable params: 1,711,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "# include early stopping and reducelr\n",
    "def get_callbacks():\n",
    "    return [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "            # monitor=\"val_accuracy\",\n",
    "            patience = 10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            # monitor = \"val_accuracy\",\n",
    "            factor = 0.5,\n",
    "            patience = 3\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "# a single dense block followed by a normalization block and relu activation\n",
    "def dense_block(units, name):\n",
    "    fc = layers.Dense(units)\n",
    "    norm = layers.LayerNormalization()\n",
    "    act = layers.Activation(\"relu\")\n",
    "    drop = layers.Dropout(0.5)\n",
    "    return lambda x: drop(act(norm(fc(x))))\n",
    "\n",
    "# the lstm block with the final dense block for the classification\n",
    "def classifier(lstm_units):\n",
    "    lstm = layers.LSTM(lstm_units)\n",
    "    out = layers.Dense(878, activation=\"softmax\")\n",
    "    return lambda x: out(lstm(x))\n",
    "# choose the number of nodes per layer\n",
    "encoder_units = [512, 256] # tune this\n",
    "lstm_units = 250 # tune this\n",
    "\n",
    "#define the inputs (ragged batches of time series of landmark coordinates)\n",
    "inputs = tf.keras.Input(shape=((372,1662)), ragged=True)\n",
    "\n",
    "# dense encoder model\n",
    "x = inputs\n",
    "for i, n in enumerate(encoder_units):\n",
    "    x = dense_block(n, f\"encoder_{i}\")(x)\n",
    "\n",
    "# classifier model\n",
    "out = classifier(lstm_units)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "34/34 [==============================] - 149s 4s/step - loss: 6.7691 - accuracy: 0.0056\n",
      "Epoch 2/500\n",
      "34/34 [==============================] - 161s 5s/step - loss: 6.7334 - accuracy: 0.0056\n",
      "Epoch 3/500\n",
      "34/34 [==============================] - 148s 4s/step - loss: 6.7333 - accuracy: 0.0056\n",
      "Epoch 4/500\n",
      "34/34 [==============================] - 141s 4s/step - loss: 6.7306 - accuracy: 0.0056\n",
      "Epoch 5/500\n",
      "34/34 [==============================] - 137s 4s/step - loss: 6.7280 - accuracy: 0.0056\n",
      "Epoch 6/500\n",
      "34/34 [==============================] - 129s 4s/step - loss: 6.7289 - accuracy: 0.0065\n",
      "Epoch 7/500\n",
      "34/34 [==============================] - 144s 4s/step - loss: 6.7272 - accuracy: 0.0056\n",
      "Epoch 8/500\n",
      "34/34 [==============================] - 147s 4s/step - loss: 6.7274 - accuracy: 0.0065\n",
      "Epoch 9/500\n",
      "34/34 [==============================] - 161s 5s/step - loss: 6.7287 - accuracy: 0.0065\n",
      "Epoch 10/500\n",
      "34/34 [==============================] - 170s 5s/step - loss: 6.7286 - accuracy: 0.0056\n",
      "Epoch 11/500\n",
      "34/34 [==============================] - 171s 5s/step - loss: 6.7283 - accuracy: 0.0065\n",
      "Epoch 12/500\n",
      "34/34 [==============================] - 168s 5s/step - loss: 6.7260 - accuracy: 0.0056\n",
      "Epoch 13/500\n",
      "34/34 [==============================] - 160s 5s/step - loss: 6.7273 - accuracy: 0.0056\n",
      "Epoch 14/500\n",
      "34/34 [==============================] - 171s 5s/step - loss: 6.7278 - accuracy: 0.0056\n",
      "Epoch 15/500\n",
      "34/34 [==============================] - 164s 5s/step - loss: 6.7250 - accuracy: 0.0056\n",
      "Epoch 16/500\n",
      "34/34 [==============================] - 190s 6s/step - loss: 6.7248 - accuracy: 0.0056\n",
      "Epoch 17/500\n",
      "34/34 [==============================] - 136s 4s/step - loss: 6.7258 - accuracy: 0.0056\n",
      "Epoch 18/500\n",
      "34/34 [==============================] - 158s 5s/step - loss: 6.7237 - accuracy: 0.0056\n",
      "Epoch 19/500\n",
      "34/34 [==============================] - 154s 5s/step - loss: 6.7244 - accuracy: 0.0056\n",
      "Epoch 20/500\n",
      "34/34 [==============================] - 159s 5s/step - loss: 6.7246 - accuracy: 0.0056\n",
      "Epoch 21/500\n",
      "34/34 [==============================] - 162s 5s/step - loss: 6.7240 - accuracy: 0.0056\n",
      "Epoch 22/500\n",
      "34/34 [==============================] - 173s 5s/step - loss: 6.7262 - accuracy: 0.0056\n",
      "Epoch 23/500\n",
      "34/34 [==============================] - 182s 5s/step - loss: 6.7248 - accuracy: 0.0065\n",
      "Epoch 24/500\n",
      "34/34 [==============================] - 200s 6s/step - loss: 6.7238 - accuracy: 0.0065\n",
      "Epoch 25/500\n",
      "34/34 [==============================] - 195s 6s/step - loss: 6.7213 - accuracy: 0.0065\n",
      "Epoch 26/500\n",
      "34/34 [==============================] - 185s 5s/step - loss: 6.7247 - accuracy: 0.0065\n",
      "Epoch 27/500\n",
      "34/34 [==============================] - 196s 6s/step - loss: 6.7239 - accuracy: 0.0065\n",
      "Epoch 28/500\n",
      "34/34 [==============================] - 199s 6s/step - loss: 6.7252 - accuracy: 0.0065\n",
      "Epoch 29/500\n",
      "34/34 [==============================] - 184s 5s/step - loss: 6.7252 - accuracy: 0.0065\n",
      "Epoch 30/500\n",
      "34/34 [==============================] - 193s 6s/step - loss: 6.7228 - accuracy: 0.0065\n",
      "Epoch 31/500\n",
      "34/34 [==============================] - 197s 6s/step - loss: 6.7220 - accuracy: 0.0065\n",
      "Epoch 32/500\n",
      "34/34 [==============================] - 202s 6s/step - loss: 6.7246 - accuracy: 0.0065\n",
      "Epoch 33/500\n",
      "34/34 [==============================] - 202s 6s/step - loss: 6.7206 - accuracy: 0.0065\n",
      "Epoch 34/500\n",
      "34/34 [==============================] - 198s 6s/step - loss: 6.7181 - accuracy: 0.0065\n",
      "Epoch 35/500\n",
      "34/34 [==============================] - 183s 5s/step - loss: 6.7328 - accuracy: 0.0065\n",
      "Epoch 36/500\n",
      "34/34 [==============================] - 198s 6s/step - loss: 6.7251 - accuracy: 0.0065\n",
      "Epoch 37/500\n",
      "34/34 [==============================] - 198s 6s/step - loss: 6.7201 - accuracy: 0.0065\n",
      "Epoch 38/500\n",
      "34/34 [==============================] - 196s 6s/step - loss: 6.7101 - accuracy: 0.0075\n",
      "Epoch 39/500\n",
      "34/34 [==============================] - 198s 6s/step - loss: 6.7179 - accuracy: 0.0065\n",
      "Epoch 40/500\n",
      "34/34 [==============================] - 185s 5s/step - loss: 6.7174 - accuracy: 0.0084\n",
      "Epoch 41/500\n",
      "34/34 [==============================] - 198s 6s/step - loss: 6.7282 - accuracy: 0.0084\n",
      "Epoch 42/500\n",
      "34/34 [==============================] - 197s 6s/step - loss: 6.7099 - accuracy: 0.0075\n",
      "Epoch 43/500\n",
      "11/34 [========>.....................] - ETA: 1:28 - loss: 6.7067 - accuracy: 0.0057"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[39m# fit the model with 100 epochs iteration\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mfit(new_x,\n\u001b[0;32m      8\u001b[0m           y,\n\u001b[0;32m      9\u001b[0m         \u001b[39m#   callbacks = get_callbacks(),\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m           epochs \u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = optimizers.RMSprop(learning_rate = 0.000001)\n",
    "\n",
    "model.compile(optimizer=\"Adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# fit the model with 100 epochs iteration\n",
    "model.fit(new_x,\n",
    "          y,\n",
    "        #   callbacks = get_callbacks(),\n",
    "          epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 17s 1s/step - loss: 5.2433 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 10s 1s/step - loss: 5.2421 - categorical_accuracy: 0.0050\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 10s 1s/step - loss: 5.2418 - categorical_accuracy: 0.0100\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 11s 1s/step - loss: 5.2416 - categorical_accuracy: 0.0100\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 11s 2s/step - loss: 5.2415 - categorical_accuracy: 0.0100\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 11s 2s/step - loss: 5.2414 - categorical_accuracy: 0.0100\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 11s 2s/step - loss: 5.2412 - categorical_accuracy: 0.0100\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 12s 2s/step - loss: 5.2411 - categorical_accuracy: 0.0100\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 13s 2s/step - loss: 5.2410 - categorical_accuracy: 0.0050\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 13s 2s/step - loss: 5.2409 - categorical_accuracy: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9624c6fe0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(new_x, y, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
